[
    {
        "idx": 0,
        "base_model":"PPL Chunking",
        "language":"zh",
        "ppl_threshold":0,
        "chunk_length":150,
        "original_text": "中共中央政治局2月28日上午就建设更高水平平安中国进行第十九次集体学习。\n 中共中央总书记习近平在主持学习时强调，建设更高水平平安中国，事关事业兴旺发达、事关人民美好生活、事关国家长治久安。\n要坚定不移贯彻总体国家安全观，在国家更加安全、社会更加有序、治理更加有效、人民更加满意上持续用力，把平安中国建设推向更高水平。西南政法大学副校长、教授李燕同志就这个问题进行讲解，提出工作建议。中央政治局的同志认真听取讲解，并进行了讨论。习近平在听取讲解和讨论后发表重要讲话。他指出，党的十八大以来，党中央不断完善国家安全领导体制和法治体系、战略体系、政策体系，完善社会治理体系，强化社会治安整体防控，着力提高公共安全治理水平，坚决维护国家主权、安全、发展利益，成功续写了经济快速发展和社会长期稳定“两大奇迹”新篇章。适应形势任务的发展变化，平安中国建设只能加强，不能削弱。习近平强调，总体国家安全观是建设更高水平平安中国的重要遵循，必须坚定不移贯彻。各级党委和政府要坚持系统思维，进一步树立发展是硬道理、安全也是硬道理的理念，在工作中自觉把发展和安全统一起来，共同谋划、一体部署、相互促进。要坚持全国一盘棋、上下齐发力，通过抓好一地一域一业的安全为国家整体安全创造条件，通过及时有效解决一个个安全问题为国家长治久安筑牢根基。\n习近平指出，平安中国建设为了人民，也依靠人民。要不断增进民生福祉，扎实推进共同富裕，切实维护人民群众合法权益，维护社会公平正义。要完善社会治理体系、健全社会工作体制机制\nn，建设人人有责、人人尽责、人人享有的社会治理共同体。要培育自尊自信、理性平和、积极向上的社会心态，弘扬锐意进取、甘于奉献、崇尚法治、文明礼让的时代新风。习近平强调，防范化解各类风险是平安中国建设的一项重要任务。要把捍卫国家政治安全摆在首位，坚定维护国家政权安全、制度安全、意识形态安全。要完善公共安全体系，推动公共安全治理模式向事前预防转型，加强防灾减灾救灾、安全生产、食品药品安全、网络安全、\n人工智能安全等方面工作。要着力防范重点领域风险。习近平指出，建设更高水平平安中国，必须强化社会治安整体防控。要把专项治理和系统治理、依法治理、综合治理、源头治理结合起来，发展壮大群防群治力量，筑起真正的铜墙铁壁。要坚持和发展新时代“枫桥经验”，推进信访工作法治化，及时有效化解各种矛盾纠纷。习近平强调，党的领导是平安中国建设的根本保证。要始终坚持党中央对国家安全工作、对政法工作的绝对领导，充分发挥各级党委在平安建设中总揽全局、协调各方的领导作用。\n 要加强正面宣传和舆论引导，注重运用现代科技手段提高社会治理效能。要锻造忠诚干净担当的新时代政法铁军。"
    },
    {
        "idx": 1,
        "base_model":"PPL Chunking",
        "language":"en",
        "ppl_threshold":0,
        "chunk_length":150,
        "original_text": "Symbolic Regression (SR) aims to discover a set of underlying symbolic analytical functions from a set of empirical observations and is often approached as a blackbox optimization problem. Major SR methodologies include GP-based methods [1]. GP represents programs as syntax trees or instruction sequences and utilizes genetic operators such as mutation and crossover to evolve new programs. They maintain a population by eliminating unfit solutions, with the fitness function guiding the selection process and indirectly leading to population improvements over many generations. Recently, deep learning methods have gained increased attention in SR. Early research [2] modified a fully connectedeedforward neural network by substituting the activation function and subsequently refining the expressions using LASSO. However, they learned a new neural network model from scratch for each problem, significantly extending the search time. To address this challenge, large-scale pre-trained models, especially transformers [3]–[6], were trained on extensive selfgenerated datasets comprising pairs of sampled data points and their corresponding mathematical expressions. These models autoregressively predict the token of a mathematical symbol, essentially forming the “skeleton” of the expression. To optimize the constants, the expressions were further refined using the Broyden–Fletcher (Goldfarb) Shanno algorithm [7]. These pretrained models capitalize on patterns identified during the extensive pretraining phase, exhibiting robust generalization performance on unseen problems. While the pretrained models can generate high-quality solutions quickly during the inference phase, they fail to achieve the final fitting accuracy of GP in well-known SR benchmarks [8], [9] due to their lack of refinement mechanisms to the specific problem. With its powerful global search capability, GP excels in exploring large and complex search spaces, making it a valuable tool for SR. A population-based approach can escape the local optima more effectively and offer resilience against noise and environmental changes. Moreover, the flexibility of GP enables it to discover interpretable solutions that traditional machine learning models might overlook. However, GP also presents certain challenges. Its high computational intensity can make training costly, particularly for large-scale problems. Since GP algorithms often start with randomly initialized populations, they may require extensive exploration before converging to high-quality solutions, especially in high-dimensional settings. Furthermore, unlike deep learning methods, which benefit from reusing various forms of learned knowledge (e.g., model weights, learned representations) [3], GP does not naturally incorporate past experiences in the same manner. Consequently, while GP can adapt flexibly to new problems, its scalability with increased data access may not be as direct as approaches that continuously refine models based on large datasets. To improve these aspects, the strength of the pretrained autoregressive model is leveraged to facilitate the GP process. Through the inference of the model, well-structured mathematical expressions were generated based on the numerical input data owing to their large-scale pretraining. We propose a Pretrained model Guided Genetic Programming (PGGP) method to assist in the initialization and mutation of the GP. Specifically, the Transformer model was used to provide a good starting point for the GP. Thus, the prediction solutions from the neural network can be improved through a subsequentevolutionary search. Furthermore, the pre-trained Transformer guided the intermediate GP process. The proposed mutation operator can rapidly generate a subtree to achieve the alternative regression goal in an end-to-end manner, making the necessary adjustments to refine the evolving solutions. While there is extensive research combining evolutionary algorithms with deep learning models, most of them evolve the population and train neural networks simultaneously [10][14]. In contrast, our method utilizes the knowledge of largescale pretrained models without the associated computational costs of training from scratch.The main contributions of this paper are as follows: • This paper introduces new initialization and mutation operators based on a pretrained Transformer model. To the best of our knowledge, this is the first study to introduce a pretrained autoregressive model to enhance the search efficiency of GP. • By employing the Transformer’s predictions to directly suggest candidate expressions that approximate the desired semantics, our approach eliminates the need for auxiliary libraries and reduces the reliance on brute-force exploration of a vast solution space. Furthermore, the ability of the Transformer to learn canonical or more compact representations from its training data enables it to generate simpler candidate solutions, effectively mitigating the issue of tree-bloating. • A variety of SR benchmark datasets are chosen to comprehensively evaluate performance. The results demonstrate that the PGGP not only outperforms the comparison methods in terms of fitting accuracy but also produces solutions whose lower complexity could enhance interpretability. In addition, we found that increasing the number of input data points further enhanced the performance of our method, which is not observed in conventional GP methods. The remainder of this paper is organized as follows. In Section II, we introduce the research on semantic mutations in GP, autoregressive models for solving SR, and related studies on combining GP and deep learning. Section III introduces our framework and the proposed operators in detail. Section IV presents the experimental setup used to evaluate the proposed PGGP and baseline methods. Section V presents the empirical results, including comprehensive analyses that compare the proposed method with existing approaches. Finally, we conclude the paper and discuss future work in Section VI.A. Semantic Mutations  Semantics refer to the phenotype of an individual, typically calculated by its output across various input cases. This contrasts with traditional GP, which manipulates individuals only at a purely syntactic level [15]. Semantic mutation leverages this semantic knowledge to guide the mutation process, thereby enhancing the effectiveness of each mutation in improving fitness. In recent years, semantic mutations have gained increasing attention owing to the efficient search capabilities of unimodal landscapes in the semantic space [16]. This theory leverages semantic information to guide the mutation processes of individuals. Semantic mutations can be classified into two main types: geometric and approximate [15].  In geometric semantic mutations, the semantics of possible solutions form a semantic space, and the underlying semantic space is searched directly through a convex combination of semantics from the parent population to approximate the target semantics [17] accurately. To improve the efficiency of the search process, Pietropolli et al. [18] employed a local search strategy to enhance the effectiveness of geometric semantic approximations by optimizing the parameters of the geometric semantic GP (GSGP) using an Adam gradient optimizer. However, owing to the high-dimensional sparse features of the semantic space, it is challenging to directly reach target semantics through a linear combination of semantic points. To address this challenge, Krawiec and Pawlak [19] introduced semantic backpropagation, which demonstrated efficient performance in navigating these complexities. Although geometric semantics enhance accuracy, they also cause significant bloating and overfitting issues. Early efforts by Vanneschi et al. [20] led to the development of a new GSGP system for efficient implementation. This system uses pointers to represent the population, generating offspring through a combination of parent pointers instead of direct copying. Although this method conserves memory space and reduces computational load, it does not mitigate the bloating issue. To decrease program size, Nguyen et al. [21] developed a method that generates small random trees and minimizes the semantic distance through parameter fitting, thereby obtaining smaller replacement subtrees. Chen et al. [22] proposed new angle-driven search operators to enhance the generalization capabilities of semantic GP. Similarly, Castelli et al. [23] introduced a multi-scheme optimization GP that searches for a globally optimal solution by minimizing the angle between a pair of trees in the error space, where the point of a tree in the error space is obtained by subtracting the target semantics from the corresponding point in the semantic space. In contrast to GSGP, which is essentially a linear combination of random programs [24], semantic approximation does not perform geometric operations directly in the semantic space. Instead, it minimizes the fitness value by directly searching for the subtrees closest to the target semantics from existing libraries. Uy et al. [25] employed semantic similarity as a criterion to fully explore the syntax space by exchanging the subtree of an individual with semantically similar but syntactically different subtrees from other parents, which smoothens the movement in the search space. On the basis of this method, Nguyen et al. [26] introduced a new method to search for semantically similar subtrees for replacement, using multiple subtree generation trials to improve semantic locality. They also suggested using autocorrelation functions and information content to describe the fitness landscape and thoroughly explored the impact of semantic mutations and standard mutation operators on the fitness landscape. Moreover, following the proposal of semantic backpropagation, Pawlak et al. [27] proposed a Random Desired Operator (RDO), that searches for suitable subtrees to approximate the desired subsemantics from an established semantics library. Subsequently, new methods based on the RDO operator were proposed. Huynh et al. [28] employed mixed-integer linear programming to search for compact and accurate expressions."
    },
    {
        "idx": 2,
        "base_model":"PPL Chunking",
        "language":"en",
        "ppl_threshold":0,
        "chunk_length":150,
        "original_text":"Following the emergence of ChatGPT (OpenAI, 2022), enthusiasm for large language models (LLMs) has escalated globally. The release of the Llama series (Touvron et al., 2023) has further ignited interests within the open-source community, particularly regarding GPT-level local LLMs. Recently, Claude-3 Opus (Anthropic, 2024) and GPT-4o (omni) (OpenAI, 2024), the updated model for ChatGPT, have ascended to the pinnacle of the Chatbot Arena (Chiang et al., 2024) in quick succession. This platform is well-regarded for its human evaluations of LLMs. Moreover, Llama-3 (AI@Meta, 2024) has emerged as the state-of-the-art open-weight model series, narrowing the performance gap with leading proprietary models and widely acknowledged as GPT-4–level. An increasing number of competitive LLMs are now pursuing advancements similar to those made by the GPT series from OpenAI. Many of these models, including Qwen (Bai et al., 2023a), Mistral (Jiang et al., 2023a), Gemma (Mesnard et al., 2024), etc., have been released in an open-weight manner. Over recent months, we have successively introduced the Qwen series (Bai et al., 2023a) and progressed to Qwen1.5 (Qwen Team, 2024a). In the meantime, we have unveiled the vision-language model Qwen-VL (Bai et al., 2023b), and launched the audio-language model Qwen-Audio (Chu et al., 2023). In this work, we introduce the newest addition to the Qwen family of large language models and large multimodal modles: Qwen2. Qwen2 is a series of LLMs, grounded in the Transformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model series encompasses foundational, i.e., base language models, pre-trained but unaligned to human preferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction following datasets suitable for chat and agent purposes. Our release comprises four dense models with parameter counts of 0.5 billion, 1.5 billion, 7 billion, and 72 billion, plus a Mixture-of-Experts (MoE) model with 57 billion parameters, of which 14 billion are activated for each token. The smaller models, specifically Qwen2-0.5B and Qwen2-1.5B, are designed for easy deployment on portable devices such as smartphones, earphones, and smart glasses. Conversely, the larger models cater to deployment across GPUs of varying scales. All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. This enrichment is hypothesized to improve reasoning abilities of LLMs. Regarding post-training, all models underwent supervised fine-tuning and direct preference optimization (DPO, Rafailov et al., 2023), aligning them with human preferences through learning from human feedback. This process endows the models with the capability to follow instructions effectively."
    }
    
]